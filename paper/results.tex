\section{Results and Conclusions}

This section contains the main results table for the 6 main data trial runs as described in the data mining section
of this paper. All results are expressed in mean absolute error (MAE). The MAE is calculated when uploading potential
results to the Kaggle website competition. The MAE is exactly what is sounds like, in that it is the average
"dollars off" between our estimate created using our data mining model and the actual answer known only by Kaggle.\\

\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c}
\hline
Free Text & Logarithmic & Square Root\\
\hline
\hline
None & 12712.17 & 13293.12\\
All & 7350.44 & 7175.04\\
Keywords (local) & 8149.91 & 7744.84\\
Keywords (global) & 8360.31544 & 8135.83 \\
\hline
\end{tabular}
\end{center}
\caption{Results from main data trial runs.\label{tab:results}}
\end{table}

As can be seen in the table above, the baseline scenario of no bag of words used in the data mining process yielded
by far the worst results. This was largely expected based on the nature of the Kaggle competition. If data mining
was better without bag of word usage, these types of challenges may not be ever needed as the simplicity of data
mining would be trivial. Where the power and knowledge of this competition really comes from is the use of the
bag of words or free text columns in the training instances and how they can be used to better the results of any
data mining predictions.\\

The next main data trial run was that on the full bag of words or free text columns inside the VW data mining tool.
These results are vastly superior to those of the non free text trials. This was also largely to be expected. Some
notes to take away from these results are as follows. For one, the use of all words in free text columns and the
VW data mining tools using the gradient descent algorithm scored a lower MAE than the benchmark set by Kaggle
which was a random forest algorithm. The Kaggle benchmark with the random forest was 7536.29 which was beat
by the full bag of words run we preformed. This already is showing promise for the use of the VW tool over a standard
tool such as Weka which may just be able to load the data and preform analysis. The Kaggle random forest benchmark
was also said to take 1.5 hours on a 2.7GHz processor the 8 cores and 8GB of RAM. The VW tool took around 5 minutes
to fully complete its run cycle. The last thing to note from this full bag of words trial is that it put our
team into 108th spot on the Kaggle leader-boards for this challenge out of 253 participating teams. This is a large
success for this project.\\

The final result for the main data trial sets was that of the keywords (local) from the keyword selection of the bag
of words columns in the training instances. We were surprised that the keywords did not help create a better solution
than just a full bag of words. If anything the local keywords should have helped alleviate pressure from words
such as "the", "it", "and" and so on. The issue may lie in the fact that using keywords will break up key sentences
that are more telling than keywords. Perhaps this would be worth exploring in the future.\\

As part of the last ditch effort of keywords, the global keyword trials can be seen in the keywords (global) row.
These results were actually slightly worse than local keywords. A reminder, the difference between global and local
was that global keywords were examined over the entire training instances and then limited to the top 500 keywords
based on occurrence. Again the limiting of words would seem like a better idea than using all words in the free
text fields, however, these limitations use less vector weights which could ultimately be the reason for the 
worse final results.\\

In conclusion, we experimented with various trial runs of gradient descent in VW with different data parsing
on free text fields. We eventually discovered that the linear gradient descent algorithm works better with
a larger collection of words for a higher number of weight vectors We have presented various way in which
keywords can be extracted from CSV files and used for data mining practices. We have also drawn the conclusion
that bag of words algorithms prefer larger bag of words compared to smaller more refined set of words.