\section{Data Mining}
\label{sec:dm}

For this project, to reiterate, we are attempting to predict job salaries given some meta information in both categorical and free text forms
about the job. The training set provided by Kaggle to issue this challenge is as astounding 244,000 training instances. This being the case,
Weka was not an appropriate solution to use as a data mining tool as holding this large amount of data in main memory while preforming standard
data mining operatings such as SVM on it would have been devistating. Another issue with this size of data are run times, especially when 
multiple attempts and different data manipulations will need to be preformed. With these issues present, it is important to either write
our own algorithms for speed and size, or use a pre existing tool to handle these scenarios for us. We went with the latter option.\\

Solving these issues, we found the data mining tool known as Vowpal Wabbit (VW). VW is a command line based data mining tool originally
created by Yahoo! Research but was later and more recently sponsored by Microsoft Research. VW was designed to be a fast and lightweight
tool from the beginning and accepting of extremely large data sets. VW has a large array of availible data mining algorithms, however,
its most prominent algorithm (and algorithm used for this project) is know as the sparse gradient descent (GD) on a loss funtion.\\

The gradient descent algorithm is a similar structure to the type of gradient descent used in the logistic regression which was taught
in the SENG474 data mining class. The main difference here is the use of the weight vector in VW's gradient descent. The weight vector
that VW uses has $2^b$ weights (where b is the specified by the b option on the command line). What this really means is that each features
as show in the data preperation section of the VW input, gets hashed to a particular weight in the vector $[0, 2^b-1]$. This allows each
feature to have its own particular weight in the gradient descent algorithm. There are some complex working of what happens when two
features get hashed to the same weight, but we do not feel the need to get into that in the paper as our feature count was low enough
to easily avoid this from happening. (We get roughtly 240,000 features in the largest scenarios.) For the loss function of VW, we decided
to use the squared loss function. Given a prediction p and a label y a loss function measures the discrepancy between the data model's
predicted outcome and the desired outcome. We selected the squared loss function because it has easier input requirements (no real
restrictions) than say logistic loss which requires labels of +1 or -1, as well as squared loss allows for a continuous prediction model
as opposed to binary predictions..\\

VW comes with its own unique input format as was described in the data preperation section of this paper. However what was not fully
explained was the flexibility of the input and how the input's features are intrepreted in the data mining process. (Refer to Equation 1)
for the following explination.) The label is the most important feature of the input, which is the class attribute of the training
instance. Since VW is expecting a normalized input in terms of the distribution of class values in the training instance, it is important
for this tool that the training instance be normalized with either the logarithm function or the square root funtion before running.\\

For the actual data mining process, 6 main data trials were given as input to the VW program. These trials and their data varried as
follows. First, a simplistic approach was taken by excluding all bag of words columns in the training instances. Bag of words form
the most complex data mining rules and therefore are exlcuded to create a base line analysis of where the tool will finish without
any of the added complexity. This data set trial was used with both the logarithmic first and square root functions of data normalization.
(All data trials from now on are used once with logarithmic and once with square root normalization.) These results of these base line
trials runs can be found in Table 5.1 in the last section of the paper. The next trial to be run on the data mining algorithm (gradient
descent) was what we call the full bag of words run. Here, all columns that are bag of words and not categorical were used in addition
to all categorical columns. The main note here is that the bag of words columns were pre processed in order to eliminate numbers, 
abbreviations, or any other text that cannot be classified in a standard dictionary. This pre processing only leaves English words
in the free text columns of the training instances. The results of this trial run can again be found in Table~\ref{tab:main}. The 
final, and most complex trial run against the gradient descent algorithm was what we are calling bag of keywords. As the title of
this trial gives away, we were only concerned with keywords in the free text columns of the training instances. In order to
get the keywords of a particular bag of text, the python library known as Topia\footnote{https://pypi.python.org/pypi/topia.termextract/}
was used. Topia is a natural language processor library which can (with some flexibility) extract the keywords from a block of text.
An example of keywords are as follows. The text "The fox can't jump over the fox's tail." in Topia yields the results:
[('tail', 1, 1), ('fox', 2, 1)]. As it can easily be seen, the keywords of tail and fox have been extracted from the main body of
text as keywords. What is less obviously are the remaining digits provided by Topia. This is where the flexibility of Topia comes into
play. In each triple, the keyword, the occurances, and the number of words in the keyword are given. This being the case, we are
able to set thresholds on Topia's extractions in order to limit the keywords selected. We can limit the keywords to only those
repeated in the text body at least once, or those that are composed of at least two words and so on. However, for simplicity of
our main data trials, we allowed topia the lowest of thresholds for keyword extractiong, being no repeat and single words. This
allowed us to again create a base line measurement of keyword usage in the data mining process. Again pre processing was done on
the training instances in order to limit the free text columns to key words only. The results of this baseline keyword run
in VW can be seein in Table~\ref{tab:main}.